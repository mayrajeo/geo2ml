[
  {
    "objectID": "data.postprocessing.html",
    "href": "data.postprocessing.html",
    "title": "Postprocessing",
    "section": "",
    "text": "First the commonly used NMS with bounding boxes, that prioritizes either confidence score (default) or bounding box area.\n\nsource\n\n\n\n non_max_suppression_fast (boxes, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort boxes by score (default) or area\nNon-max suppression can in theory be applied also on polygons, but it hasn’t been used in any publications as far as I know.\nIf non_max_suppression_poly is used to eliminate polygons, threshold might need to be smaller than typical value of 0.7 that is used.\n\nsource\n\n\n\n\n poly_IoU (poly_1:shapely.geometry.polygon.Polygon,\n           poly_2:shapely.geometry.polygon.Polygon)\n\nCalculate IoU for two shapely Polygons\n\nsource\n\n\n\n\n non_max_suppression_poly (geoms, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nDo non-max suppression for shapely Polygons in geoms. Can be sorted according to area or score\nSome utils to run above functions to GeoDataFrames\n\nsource\n\n\n\n\n do_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.7,\n         crit='score')\n\nPerform non-max suppression for bounding boxes using nms_threshold to gdf\n\ndef do_poly_nms(gdf:gpd.GeoDataFrame, nms_thresh=0.1, crit='score') -&gt; gpd.GeoDataFrame:\n    \"Perform non-max suppression for polygons using `nms_threshold` to `gdf`\"\n    gdf = gdf.copy()\n    scores = gdf.score.values\n    idxs = non_max_suppression_poly(gdf.geometry.values, scores, nms_thresh, crit)\n    gdf = gdf.iloc[idxs]\n    return gdf\n\n\ndef do_min_rot_rectangle_nms(gdf:gpd.GeoDataFrame, nms_thresh=0.7, crit='score') -&gt; gpd.GeoDataFrame:\n    \"Perform non-max suppression for rotated bounding boxes using `nms_threshold` to `gdf`\"\n    gdf = gdf.copy()\n    scores = gdf.score.values\n    boxes = np.array([g.minimum_rotated_rectangle for g in gdf.geometry.values])\n    idxs = non_max_suppression_poly(boxes, scores, nms_thresh, crit)\n    gdf = gdf.iloc[idxs]\n    return gdf",
    "crumbs": [
      "Image data",
      "Postprocessing"
    ]
  },
  {
    "objectID": "data.postprocessing.html#non-maximum-suppression",
    "href": "data.postprocessing.html#non-maximum-suppression",
    "title": "Postprocessing",
    "section": "",
    "text": "First the commonly used NMS with bounding boxes, that prioritizes either confidence score (default) or bounding box area.\n\nsource\n\n\n\n non_max_suppression_fast (boxes, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort boxes by score (default) or area\nNon-max suppression can in theory be applied also on polygons, but it hasn’t been used in any publications as far as I know.\nIf non_max_suppression_poly is used to eliminate polygons, threshold might need to be smaller than typical value of 0.7 that is used.\n\nsource\n\n\n\n\n poly_IoU (poly_1:shapely.geometry.polygon.Polygon,\n           poly_2:shapely.geometry.polygon.Polygon)\n\nCalculate IoU for two shapely Polygons\n\nsource\n\n\n\n\n non_max_suppression_poly (geoms, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nDo non-max suppression for shapely Polygons in geoms. Can be sorted according to area or score\nSome utils to run above functions to GeoDataFrames\n\nsource\n\n\n\n\n do_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.7,\n         crit='score')\n\nPerform non-max suppression for bounding boxes using nms_threshold to gdf\n\ndef do_poly_nms(gdf:gpd.GeoDataFrame, nms_thresh=0.1, crit='score') -&gt; gpd.GeoDataFrame:\n    \"Perform non-max suppression for polygons using `nms_threshold` to `gdf`\"\n    gdf = gdf.copy()\n    scores = gdf.score.values\n    idxs = non_max_suppression_poly(gdf.geometry.values, scores, nms_thresh, crit)\n    gdf = gdf.iloc[idxs]\n    return gdf\n\n\ndef do_min_rot_rectangle_nms(gdf:gpd.GeoDataFrame, nms_thresh=0.7, crit='score') -&gt; gpd.GeoDataFrame:\n    \"Perform non-max suppression for rotated bounding boxes using `nms_threshold` to `gdf`\"\n    gdf = gdf.copy()\n    scores = gdf.score.values\n    boxes = np.array([g.minimum_rotated_rectangle for g in gdf.geometry.values])\n    idxs = non_max_suppression_poly(boxes, scores, nms_thresh, crit)\n    gdf = gdf.iloc[idxs]\n    return gdf",
    "crumbs": [
      "Image data",
      "Postprocessing"
    ]
  },
  {
    "objectID": "data.tiling.html",
    "href": "data.tiling.html",
    "title": "Tiling",
    "section": "",
    "text": "Example area looks like this\nfrom rasterio import plot as rioplot\nimport matplotlib.pyplot as plt\nf = gpd.read_file('example_data/R70C21.shp')\nf.head()\nf['label_id'] = f.apply(lambda row: 2 if row.label == 'Standing' else 1, axis=1)\nf.to_file('example_data/R70C21.shp')\nraster = rio.open('example_data/R70C21.tif')\nrioplot.show(raster)\nWith rasterio.plot it is a lot easier to visualize shapefile and raster simultaneously\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\nf.plot(ax=ax, column='label_id')",
    "crumbs": [
      "Image data",
      "Tiling"
    ]
  },
  {
    "objectID": "data.tiling.html#tiling",
    "href": "data.tiling.html#tiling",
    "title": "Tiling",
    "section": "Tiling",
    "text": "Tiling\n\nsource\n\nTiler\n\n Tiler (outpath, gridsize_x:int=400, gridsize_y:int=400,\n        overlap:tuple[int,int]=(100, 100))\n\nHandles the tiling of raster and vector data into smaller patches that each have the same coverage.\n\ntiler = Tiler(outpath='example_data/tiles', gridsize_x=240, gridsize_y=180, overlap=(120, 90))\n\n\nsource\n\n\nTiler.tile_raster\n\n Tiler.tile_raster (path_to_raster:pathlib.Path|str,\n                    allow_partial_data:bool=False)\n\nTiles specified raster to self.gridsize_x times self.gridsize_y grid, with self.overlap pixel overlap\n\ntiler.tile_raster('example_data/R70C21.tif')\n\n\n\n\n\nsource\n\n\nTiler.tile_vector\n\n Tiler.tile_vector (path_to_vector:pathlib.Path|str,\n                    min_area_pct:float=0.0, gpkg_layer:str=None,\n                    output_format:str='geojson')\n\nTiles a vector data file into smaller tiles. Converts all multipolygons to a regular polygons. min_area_pct is be used to specify the minimum area for partial masks to keep. Default value 0.0 keeps all masks.\nIf output_format is geojson, the resulting files are saved into outpath/vectors. If output_format is gpkg, then each file is saved as a layer in outpath/vectors.gpkg.\n\ntiler.tile_vector('example_data/R70C21.shp', min_area_pct=.2)\n\n\n\n\n\ntiler.tile_vector(Path('example_data/R70C21.shp'), min_area_pct=.2, output_format='gpkg')\n\n\n\n\n\ntest_eq(len(fiona.listlayers('example_data/tiles/vectors.gpkg')), len(os.listdir('example_data/tiles/vectors/')))\n\n\nsource\n\n\nTiler.tile_and_rasterize_vector\n\n Tiler.tile_and_rasterize_vector (path_to_raster:pathlib.Path|str,\n                                  path_to_vector:pathlib.Path|str,\n                                  column:str, gpkg_layer:str=None,\n                                  keep_bg_only:bool=False)\n\nRasterizes vectors based on tiled rasters. Saves label map to self.outpath. By default only keeps the patches that contain polygon data, by specifying keep_bg_only=True saves also masks for empty patches.\n\ntiler.tile_and_rasterize_vector('example_data/R70C21.tif', Path('example_data/R70C21.shp'), \n                                column='label')\n\n\n\n\n\ntiler.tile_and_rasterize_vector('example_data/R70C21.tif', 'example_data/R70C21.shp', \n                                column='label', keep_bg_only=True)\n\n\n\n\n\nwith rio.open('example_data/tiles/rasterized_vectors/R1C3.tif') as i: im = i.read()\nplt.imshow(im[0])",
    "crumbs": [
      "Image data",
      "Tiling"
    ]
  },
  {
    "objectID": "data.tiling.html#reversing",
    "href": "data.tiling.html#reversing",
    "title": "Tiling",
    "section": "Reversing",
    "text": "Reversing\n\nsource\n\nuntile_vector\n\n untile_vector (path_to_targets:pathlib.Path|str,\n                outpath:pathlib.Path|str,\n                non_max_suppression_thresh:float=0.0,\n                nms_criterion:str='score')\n\nCreate single GIS-file from a directory of predicted .shp or .geojson files\n\nsource\n\n\ncopy_sum\n\n copy_sum (merged_data, new_data, merged_mask, new_mask, **kwargs)\n\nMake new pixels have the sum of two overlapping pixels as their value. Useful with prediction data\n\nsource\n\n\nuntile_raster\n\n untile_raster (path_to_targets:pathlib.Path|str,\n                outfile:pathlib.Path|str, method:str='first')\n\nMerge multiple patches from path_to_targets into a single raster`\nUntile shapefiles and check how they look\n\nuntile_vector(f'example_data/tiles/vectors', outpath='example_data/untiled.geojson')\n\n\n\n\n81 polygons\n\n\n\nuntile_vector(f'example_data/tiles/vectors.gpkg', outpath='example_data/untiled_gpkg.geojson')\n\n\n\n\n81 polygons\n\n\nPlot with the tiled grid.\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\ntiler.grid.exterior.plot(ax=ax)\nuntiled.plot(ax=ax, column='label_id', facecolor='none', edgecolor='black')\n\n\n\n\n\n\n\n\nIf allow_partial_data=False as is the default behaviour, tiling is done only for the area from which full sized patch can be extracted. With allow_partial_data=True, windows can “extend” to empty areas. This is useful with inference, when predicted areas can have wonky dimensions.\n\ntiler_part = Tiler(outpath='example_data/tiles_partial', gridsize_x=240, gridsize_y=180, overlap=(120, 90))\ntiler_part.tile_raster('example_data/R70C21.tif', allow_partial_data=True)\n\n\n\n\n\ntiler_part.tile_vector('example_data/R70C21.shp', min_area_pct=.2)\ntiler_part.tile_vector('example_data/R70C21.shp', min_area_pct=.2, output_format='gpkg')\ntest_eq(len(os.listdir('example_data/tiles_partial/vectors/')), len(fiona.listlayers('example_data/tiles_partial/vectors.gpkg')))\n\n\n\n\n\n\n\nUntile shapefiles and check how they look\n\nuntile_vector(f'example_data/tiles/vectors', outpath='example_data/untiled.geojson')\n\n\n\n\n81 polygons\n\n\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\nuntiled.plot(ax=ax, column='label_id', facecolor='none', edgecolor='black')\n\n\n\n\n\n\n\n\nPlot with the tiled grid.\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\ntiler.grid.exterior.plot(ax=ax)\nuntiled.plot(ax=ax, column='label_id')\n\n\n\n\n\n\n\n\nuntile_raster can be used to mosaic all patches into one.\n\nuntile_raster('example_data/tiles/rasterized_vectors/', 'example_data/tiles/mosaic_first.tif', \n              method='first')\n\n\nwith rio.open('example_data/tiles/mosaic_first.tif') as mos: mosaic = mos.read()\nplt.imshow(mosaic[0])\n\n\n\n\n\n\n\n\nBy specifying method as sum it’s possible to collate predictions and get the most likely label for pixels\n\nuntile_raster('example_data/tiles/rasterized_vectors/', 'example_data/tiles/mosaic_sum.tif',\n              method='sum')\n\n\nwith rio.open('example_data/tiles/mosaic_sum.tif') as mos: mosaic = mos.read()\nplt.imshow(mosaic[0])",
    "crumbs": [
      "Image data",
      "Tiling"
    ]
  },
  {
    "objectID": "examples_yolo.html",
    "href": "examples_yolo.html",
    "title": "YOLOv8 workflow",
    "section": "",
    "text": "import rasterio as rio\nimport geopandas as gpd\nfrom pathlib import Path\nimport rasterio.plot as rioplot\nimport matplotlib.pyplot as plt\npath_to_data = Path('workflow_examples/')\ntrain_raster = path_to_data/'104_28_Hiidenportti_Chunk1_orto.tif'\ntrain_shp = path_to_data/'104_28_Hiidenportti_Chunk1_orto.geojson'\ntest_raster = path_to_data/'104_42_Hiidenportti_Chunk5_orto.tif'\ntest_shp = path_to_data/'104_42_Hiidenportti_Chunk5_orto.geojson'\nExample data is RGB UAV imagery from Hiidenportti, and the task is to detect and segment different deadwood types. The reference data are annotated as polygons, and target column is layer.\nTraining area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(10,3))\nwith rio.open(train_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntrain_gdf = gpd.read_file(train_shp)\ntrain_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Train area')\nplt.tight_layout()\nplt.show()\nAnd test area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(5,3))\nwith rio.open(test_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntest_gdf = gpd.read_file(test_shp)\ntest_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Test area')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_yolo.html#install-required-dependencies",
    "href": "examples_yolo.html#install-required-dependencies",
    "title": "YOLOv8 workflow",
    "section": "Install required dependencies",
    "text": "Install required dependencies\nultralytics requires at least Python 3.8 and PyTorch 1.7. First install PyTorch according to instructions found here, then install ultralytics with pip install ultralytics.",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_yolo.html#create-yolo-format-dataset",
    "href": "examples_yolo.html#create-yolo-format-dataset",
    "title": "YOLOv8 workflow",
    "section": "Create YOLO-format dataset",
    "text": "Create YOLO-format dataset\nIn this example, the data are split into 320x320 pixel tiles with no overlap. Also set the min_bbox_area to 8 pixels so too small objects are discarded.\n\nCLI\ngeo2ml_create_yolo_dataset \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.tif \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.geojson layer \\\nexample_data/workflow_examples/yolo/train --gridsize_x 320 --gridsize_y 320 \\\n--ann_format polygon --min_bbox_area 8\n\ngeo2ml_create_yolo_dataset \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.tif \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.geojson layer \\\nexample_data/workflow_examples/yolo/test --gridsize_x 320 --gridsize_y 320 \\\n--ann_format polygon --min_bbox_area 8\n\n\nPython\n\nfrom geo2ml.scripts.data import create_yolo_dataset\n\n\noutpath = path_to_data/'yolo'\n\ncreate_yolo_dataset(raster_path=train_raster, polygon_path=train_shp, target_column='layer',\n                    outpath=outpath/'train', save_grid=False, gridsize_x=320, gridsize_y=320,\n                    ann_format='polygon', min_bbox_area=8, allow_partial_data=True)\n\ncreate_yolo_dataset(raster_path=test_raster, polygon_path=test_shp, target_column='layer',\n                    outpath=outpath/'test', save_grid=False, gridsize_x=320, gridsize_y=320,\n                    ann_format='polygon', min_bbox_area=8, allow_partial_data=True)",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_yolo.html#dataset-structure",
    "href": "examples_yolo.html#dataset-structure",
    "title": "YOLOv8 workflow",
    "section": "Dataset structure",
    "text": "Dataset structure\nAbove creates the dataset to path_to_data/'yolo', so that it contains folders train and test. Both of these folders contain\n\nfolder images, which contains the tiled raster patches\nfolder vectors, which contain geojson-files corresponding to each file in images, if the location contains any annotations\nfolder labels, which contain the annotations in YOLO format\nfile yolo.yaml, which can be used as a template for the dataset description file",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_yolo.html#create-yolo.yaml-for-the-dataset",
    "href": "examples_yolo.html#create-yolo.yaml-for-the-dataset",
    "title": "YOLOv8 workflow",
    "section": "Create yolo.yaml for the dataset",
    "text": "Create yolo.yaml for the dataset\nCreate a yolo.yaml file using the below template. Autogeneration for this might be added later.\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3: list: [path/to_imgs1, path_to_imgs2, ..]\npath: workflow_examples/yolo/ # dataset root dir \ntrain: train\nval: test\ntest: test\n\n# Classes\nnames:\n  0: groundwood\n  1: uprightwood\nIt is advisable to use absolute path as the path: variable, as ultralytics otherwise assumes that path is relative to the datasets_dir in your settings.",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_yolo.html#train-the-model",
    "href": "examples_yolo.html#train-the-model",
    "title": "YOLOv8 workflow",
    "section": "Train the model",
    "text": "Train the model\n\nfrom ultralytics import YOLO\n\nAs an example, train yolov8n-seg.pt for 30 epochs with 640x640px images and batch size of 4 (due to GPU constraints).\n\nmodel = YOLO('yolov8n-seg.pt')\nresults = model.train(data=path_to_data/'yolo/yolo.yaml', epochs=30, imgsz=640, batch=4, \n                      optimizer='auto', project=path_to_data/'yolo/runs')\n\nUltralytics YOLOv8.0.180 🚀 Python-3.11.5 torch-2.0.1 CUDA:0 (NVIDIA RTX A2000 8GB Laptop GPU, 8192MiB)\nengine/trainer: task=segment, mode=train, model=yolov8n-seg.pt, data=workflow_examples/yolo/yolo.yaml, epochs=30, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=workflow_examples/yolo/runs, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=workflow_examples/yolo/runs/train2\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1   1004470  ultralytics.nn.modules.head.Segment          [2, 32, 64, [64, 128, 256]]   \nYOLOv8n-seg summary: 261 layers, 3264006 parameters, 3263990 gradients\n\nTransferred 381/417 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir workflow_examples/yolo/runs/train2', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n100%|██████████████████████████████████████████████████████████████| 6.23M/6.23M [00:04&lt;00:00, 1.59MB/s]\nAMP: checks passed ✅\ntrain: Scanning /mnt/d/Users/E1005164/geo2ml/nbs/workflow_examples/yolo/train/labels... 296 images, 79 b\ntrain: New cache created: /mnt/d/Users/E1005164/geo2ml/nbs/workflow_examples/yolo/train/labels.cache\nval: Scanning /mnt/d/Users/E1005164/geo2ml/nbs/workflow_examples/yolo/test/labels... 118 images, 22 back\nval: New cache created: /mnt/d/Users/E1005164/geo2ml/nbs/workflow_examples/yolo/test/labels.cache\nPlotting labels to workflow_examples/yolo/runs/train2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.001667, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to workflow_examples/yolo/runs/train2\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       1/30      1.02G      1.954      2.929      3.966      1.645         10        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.708     0.0661      0.132     0.0589       0.68     0.0572      0.117     0.0378\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       2/30      1.01G      1.857      2.211      3.052      1.532         12        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.491      0.335      0.374      0.178      0.483      0.321      0.347      0.151\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       3/30      0.96G      1.853       2.11      2.738      1.529         13        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.384      0.358       0.32       0.18      0.419      0.323      0.306      0.144\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       4/30     0.963G      1.843      2.023      2.641       1.52          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.539      0.368      0.389      0.201      0.531      0.365      0.371      0.178\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       5/30     0.963G      1.768      2.081      2.491      1.503         19        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.424      0.461      0.417      0.229       0.44      0.437      0.404      0.195\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       6/30     0.963G      1.782      2.009       2.36      1.543          9        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.446      0.463      0.443       0.24      0.453      0.465      0.438      0.191\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       7/30     0.958G      1.814      1.953       2.39      1.505          7        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.583      0.467      0.475      0.254      0.571      0.463       0.46      0.212\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       8/30     0.963G      1.719      1.826      2.307      1.479         20        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.571      0.416      0.439      0.231      0.542      0.417      0.427      0.209\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n       9/30     0.963G      1.711      1.799      2.372      1.478          3        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.494       0.43      0.437      0.251      0.473      0.409      0.401      0.197\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      10/30     0.963G      1.666       1.89      1.923      1.421         10        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.486      0.506      0.469      0.264      0.533      0.485      0.467      0.227\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      11/30     0.963G      1.729      1.813       2.12      1.482          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.486      0.495      0.481      0.263      0.477      0.481      0.462      0.224\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      12/30      0.96G       1.76      1.738      2.119      1.484          6        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575        0.6      0.535      0.557      0.309      0.592      0.522      0.538      0.266\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      13/30     0.971G      1.665      1.688      2.004       1.43         15        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.608      0.537      0.538      0.302      0.603       0.53      0.516      0.254\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      14/30     0.958G      1.654      1.766      1.967      1.398          6        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.644       0.53      0.583       0.33      0.646      0.532      0.581      0.282\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      15/30     0.958G      1.606      1.675      1.869      1.414          8        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.527      0.504      0.517      0.271      0.549      0.452      0.472      0.212\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      16/30     0.958G      1.541      1.722      1.738      1.385         10        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.532      0.544      0.533      0.308      0.526      0.542      0.503      0.244\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      17/30      0.96G      1.539      1.672      1.794      1.372         10        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.587      0.478      0.521       0.29      0.608      0.472       0.52      0.251\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      18/30     0.956G      1.516      1.637      1.752      1.349          9        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.464      0.516      0.459      0.271      0.451      0.501      0.445      0.228\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      19/30     0.958G      1.516      1.776      1.713      1.333          8        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575       0.61       0.57      0.579      0.346      0.616      0.534      0.552      0.282\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      20/30     0.956G      1.502      1.645       1.67      1.339          2        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575       0.54      0.468      0.505      0.304      0.548      0.464      0.493      0.242\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      21/30     0.958G      1.421      1.339      1.935       1.34          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.602      0.554      0.566       0.33      0.602      0.531      0.543       0.27\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      22/30     0.956G      1.466      1.358      1.722      1.363          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.644      0.617      0.636      0.377      0.639      0.604      0.616       0.31\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      23/30     0.958G       1.41      1.325      1.667      1.329          6        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.641      0.583      0.596      0.356      0.639      0.575       0.58      0.288\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      24/30     0.958G      1.373      1.287      1.697      1.285         10        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.665      0.534        0.6      0.355      0.663       0.53      0.581      0.289\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      25/30     0.956G      1.437      1.277      1.644      1.309          8        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.628       0.56      0.571      0.344      0.607      0.532      0.552      0.274\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      26/30     0.956G      1.387      1.257      1.554      1.293          1        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.564      0.578      0.581      0.356      0.549      0.562       0.56      0.288\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      27/30     0.956G      1.339      1.293      1.522      1.303          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.571      0.576      0.583      0.358      0.562      0.563      0.561      0.285\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      28/30     0.956G       1.32      1.255      1.451      1.252          2        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575       0.65      0.541        0.6      0.368      0.635      0.544      0.585      0.293\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      29/30     0.954G      1.338      1.191      1.559      1.277          4        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.639      0.561      0.612      0.364      0.613      0.562      0.594      0.298\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      30/30     0.956G      1.298      1.234      1.599      1.246          5        640: 100%|█████████\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.661      0.557       0.59      0.358      0.652       0.55       0.57       0.29\n\n30 epochs completed in 0.349 hours.\nOptimizer stripped from workflow_examples/yolo/runs/train2/weights/last.pt, 6.8MB\nOptimizer stripped from workflow_examples/yolo/runs/train2/weights/best.pt, 6.8MB\n\nValidating workflow_examples/yolo/runs/train2/weights/best.pt...\nUltralytics YOLOv8.0.180 🚀 Python-3.11.5 torch-2.0.1 CUDA:0 (NVIDIA RTX A2000 8GB Laptop GPU, 8192MiB)\nYOLOv8n-seg summary (fused): 195 layers, 3258454 parameters, 0 gradients\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P     \n                   all        140        575      0.644      0.617      0.636      0.378      0.636      0.609      0.617       0.31\n            groundwood        140        446      0.571      0.529      0.532      0.291      0.564      0.522      0.496      0.203\n           uprightwood        140        129      0.716      0.705       0.74      0.464      0.708      0.696      0.738      0.418\nSpeed: 1.3ms preprocess, 23.2ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to workflow_examples/yolo/runs/train2",
    "crumbs": [
      "Examples",
      "YOLOv8 workflow"
    ]
  },
  {
    "objectID": "examples_coco.html",
    "href": "examples_coco.html",
    "title": "COCO workflow",
    "section": "",
    "text": "import rasterio as rio\nimport geopandas as gpd\nfrom pathlib import Path\nimport rasterio.plot as rioplot\nimport matplotlib.pyplot as plt\npath_to_data = Path('workflow_examples/')\ntrain_raster = path_to_data/'104_28_Hiidenportti_Chunk1_orto.tif'\ntrain_shp = path_to_data/'104_28_Hiidenportti_Chunk1_orto.geojson'\ntest_raster = path_to_data/'104_42_Hiidenportti_Chunk5_orto.tif'\ntest_shp = path_to_data/'104_42_Hiidenportti_Chunk5_orto.geojson'\nExample data is RGB UAV imagery from Hiidenportti, and the task is to detect and segment different deadwood types. The reference data are annotated as polygons, and target column is layer.\nTraining area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(10,3))\nwith rio.open(train_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntrain_gdf = gpd.read_file(train_shp)\ntrain_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Train area')\nplt.tight_layout()\nplt.show()\nAnd test area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(5,3))\nwith rio.open(test_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntest_gdf = gpd.read_file(test_shp)\ntest_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Test area')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#install-required-dependencies",
    "href": "examples_coco.html#install-required-dependencies",
    "title": "COCO workflow",
    "section": "Install required dependencies",
    "text": "Install required dependencies\nIn order to install detectron2, follow the instructions provided here.",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#create-coco-format-dataset",
    "href": "examples_coco.html#create-coco-format-dataset",
    "title": "COCO workflow",
    "section": "Create COCO-format dataset",
    "text": "Create COCO-format dataset\nIn this example, the data are split into 256x256 pixel tiles with no overlap. Also set the min_bbox_area to 8 pixels so too small objects are discarded.",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#cli",
    "href": "examples_coco.html#cli",
    "title": "COCO workflow",
    "section": "CLI",
    "text": "CLI\ngeo2ml_create_coco_dataset \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.tif \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.geojson layer \\\nexample_data/workflow_examples/coco/train example_train \\\n--gridsize_x 256 --gridsize_y 256 \\\n--ann_format polygon --min_bbox_area 8\n\ngeo2ml_create_coco_dataset \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.tif \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.geojson layer \\\nexample_data/workflow_examples/coco/test example_test \\\n--gridsize_x 256 --gridsize_y 256 \\\n--ann_format polygon --min_bbox_area 8",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#python",
    "href": "examples_coco.html#python",
    "title": "COCO workflow",
    "section": "Python",
    "text": "Python\n\nfrom geo2ml.scripts.data import create_coco_dataset\n\n\noutpath = path_to_data/'coco'\n\ncreate_coco_dataset(raster_path=train_raster, polygon_path=train_shp, target_column='layer',\n                    outpath=outpath/'train', output_format='gpkg', save_grid=False, allow_partial_data=True,\n                    dataset_name='example_train', gridsize_x=256, gridsize_y=256, \n                    ann_format='polygon', min_bbox_area=8)\n\ncreate_coco_dataset(raster_path=test_raster, polygon_path=test_shp, target_column='layer',\n                    outpath=outpath/'test',output_format='gpkg', save_grid=False, allow_partial_data=True,\n                    dataset_name='example_test', gridsize_x=256, gridsize_y=256, ann_format='polygon', min_bbox_area=8)",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#dataset-structure",
    "href": "examples_coco.html#dataset-structure",
    "title": "COCO workflow",
    "section": "Dataset structure",
    "text": "Dataset structure\nAbove creates the dataset to path_to_data/'yolo', so that it contains folders train and test. Both of these folders contain\n\nfolder images, which contains the tiled raster patches\nfolder vectors, which contain geojson-files corresponding to each file in images, if the location contains any annotations\nfile coco_polygon.json, which is the annotation file and info for the dataset",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#train-the-model",
    "href": "examples_coco.html#train-the-model",
    "title": "COCO workflow",
    "section": "Train the model",
    "text": "Train the model\n\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator, DatasetEvaluators\nimport os\n\nFirst we need to register the datasets:\n\nregister_coco_instances(name='example_train', # the name that identifies a dataset for this session\n                        metadata={}, # extra metadata, can be left as an empty dict\n                        json_file=outpath/'train/example_train.json', # Annotation file\n                        image_root=outpath/'train/images/') # directory which contains all the images\nregister_coco_instances('example_test', {}, outpath/'test/example_test.json', outpath/'test/images')\n\nAnd then modify the config file.\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"example_train\",)\ncfg.DATASETS.TEST = (\"example_test\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.TEST.EVAL_PERIOD = 100\ncfg.OUTPUT_DIR = str(outpath/'runs')\ncfg.SOLVER.MAX_ITER = 300\ncfg.MODEL.ROI_HEADS.NUM_CLASSES\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\nNext create a trainer. Here we use DefaultTrainer because of demo purposes.\n\ntrainer = DefaultTrainer(cfg)\n\nFirst the trainer must resume_or_load the checkpoint.\n\ntrainer.resume_or_load(resume=False)\n\nThen it can be trained:\n\ntrainer.train()\n\nFor evaluation we need to build an evaluator to like the following:\n\nresults = trainer.test(cfg, trainer.model, \n                       evaluators=DatasetEvaluators([COCOEvaluator('example_test', \n                                                                   output_dir=cfg.OUTPUT_DIR)]))\n\nResults are returned as OrderedDict:\n\nresults\n\nOrderedDict([('bbox',\n              {'AP': 26.524945427964656,\n               'AP50': 54.61366682355787,\n               'AP75': 23.264070383011905,\n               'APs': 25.132186571431554,\n               'APm': 21.41516566172374,\n               'APl': nan,\n               'AP-groundwood': 23.99694519893256,\n               'AP-uprightwood': 29.052945656996748}),\n             ('segm',\n              {'AP': 24.178996439781,\n               'AP50': 50.930807248720775,\n               'AP75': 19.645957078082983,\n               'APs': 19.084138095771337,\n               'APm': 30.276207887160094,\n               'APl': nan,\n               'AP-groundwood': 18.747639759369484,\n               'AP-uprightwood': 29.610353120192507})])",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_coco.html#other-libraries",
    "href": "examples_coco.html#other-libraries",
    "title": "COCO workflow",
    "section": "Other libraries",
    "text": "Other libraries\nMMDetection is another commonly used library for object detection from COCO formatted datasets. According to these instructions, below should work:\n# the new config inherits the base configs to highlight the necessary modification\n_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py\n\n# 1. dataset settings\ndataset_type = 'CocoDataset'\nclasses = ('Standing', 'Fallen')\ndata_root = /workflow_examples/coco/'\n\ntrain_dataloader = dict(\n    batch_size=2,\n    num_workers=2,\n    dataset=dict(\n        type=dataset_type,\n        metainfo=dict(classes=classes),\n        data_root=data_root,\n        ann_file='train/coco_polygon.json',\n        data_prefix=dict(img='train/images')\n    )\n)\n...",
    "crumbs": [
      "Examples",
      "COCO workflow"
    ]
  },
  {
    "objectID": "examples_unet.html",
    "href": "examples_unet.html",
    "title": "Unet workflow",
    "section": "",
    "text": "import rasterio as rio\nimport geopandas as gpd\nfrom pathlib import Path\nimport rasterio.plot as rioplot\nimport matplotlib.pyplot as plt\npath_to_data = Path('workflow_examples/')\ntrain_raster = path_to_data/'104_28_Hiidenportti_Chunk1_orto.tif'\ntrain_shp = path_to_data/'104_28_Hiidenportti_Chunk1_orto.geojson'\ntest_raster = path_to_data/'104_42_Hiidenportti_Chunk5_orto.tif'\ntest_shp = path_to_data/'104_42_Hiidenportti_Chunk5_orto.geojson'\nExample data is RGB UAV imagery from Hiidenportti, and the task is to detect and segment different deadwood types. The reference data are annotated as polygons, and target column is layer.\nTraining area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(10,3))\nwith rio.open(train_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntrain_gdf = gpd.read_file(train_shp)\ntrain_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Train area')\nplt.tight_layout()\nplt.show()\nAnd test area looks like this.\nfig, axs = plt.subplots(1,2, dpi=150, figsize=(5,3))\nwith rio.open(test_raster) as src:\n    rioplot.show(src, ax=axs[0])\ntest_gdf = gpd.read_file(test_shp)\ntest_gdf.plot(column='layer', ax=axs[1], cmap='seismic')\nplt.suptitle('Test area')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#install-required-dependencies",
    "href": "examples_unet.html#install-required-dependencies",
    "title": "Unet workflow",
    "section": "Install required dependencies",
    "text": "Install required dependencies\nSimplest way to install fastai is\nconda install -c fastchan fastai\nOther ways to install can be found here.",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#create-a-raster-dataset",
    "href": "examples_unet.html#create-a-raster-dataset",
    "title": "Unet workflow",
    "section": "Create a raster dataset",
    "text": "Create a raster dataset\nIn this example, the data are split into 224x224 pixel tiles with no overlap.",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#cli",
    "href": "examples_unet.html#cli",
    "title": "Unet workflow",
    "section": "CLI",
    "text": "CLI\ngeo2ml_create_raster_dataset \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.tif \\\nexample_data/workflow_examples/104_28_Hiidenportti_Chunk1_orto.geojson \\\nexample_data/workflow_examples/unet/train \\\n--target_column layer --gridsize_x 224 --gridsize_y 224\n\ngeo2ml_create_raster_dataset \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.tif \\\nexample_data/workflow_examples/104_42_Hiidenportti_Chunk5_orto.geojson layer \\\nexample_data/workflow_examples/unet/test \\\n--target_column layer --gridsize_x 224 --gridsize_y 224",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#python",
    "href": "examples_unet.html#python",
    "title": "Unet workflow",
    "section": "Python",
    "text": "Python\n\nfrom geo2ml.scripts.data import create_raster_dataset\n\n\noutpath = path_to_data/'unet'\n\ncreate_raster_dataset(raster_path=train_raster, mask_path=train_shp, outpath=outpath/'train',\n                      save_grid=False, target_column='layer', gridsize_x=224, gridsize_y=224)\n\ncreate_raster_dataset(raster_path=test_raster, mask_path=test_shp, outpath=outpath/'test',\n                      save_grid=False, target_column='layer', gridsize_x=224, gridsize_y=224)",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#dataset-structure",
    "href": "examples_unet.html#dataset-structure",
    "title": "Unet workflow",
    "section": "Dataset structure",
    "text": "Dataset structure\nAbove creates the dataset to path_to_data/'unet', so that it contains folders train and test. Both of these contain\n\nfolder images, which contains the tiled raster patches\nfolder mask_images, which contain the rasterized masks corresponding to files in images\nlabel_map.txt, which has the mapping between integer values and classes",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "examples_unet.html#train-the-model",
    "href": "examples_unet.html#train-the-model",
    "title": "Unet workflow",
    "section": "Train the model",
    "text": "Train the model\n\nfrom fastai.vision.all import *\n\n\nfnames = [Path(outpath/'train/images'/f) for f in os.listdir(outpath/'train/mask_images')]\n\ndef label_from_different_folder(fn, original_folder, new_folder):\n    return str(fn).replace(original_folder, new_folder)\n\ndls = SegmentationDataLoaders.from_label_func(outpath/'train/images', bs=8,\n                                              codes=['Background', 'Standing', 'Fallen'],\n                                              fnames=fnames,\n                                              label_func=partial(label_from_different_folder,\n                                                                 original_folder='images',\n                                                                 new_folder='mask_images'),\n                                              batch_tfms = [\n                                                  *aug_transforms(max_rotate=0., max_warp=0.),\n                                                  Normalize.from_stats(*imagenet_stats)\n                                              ])\n\nFine-tune a Unet that uses resnet34 as the encoder.\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(6)\n\n/home/mayrajeo/miniconda3/envs/point-eo-dev/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/mayrajeo/miniconda3/envs/point-eo-dev/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/mayrajeo/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|███████████████████████████████████████████████████████████████| 83.3M/83.3M [00:35&lt;00:00, 2.48MB/s]\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.102724\n0.063255\n00:31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.060898\n0.049642\n00:32\n\n\n1\n0.049917\n0.035729\n00:32\n\n\n2\n0.038387\n0.032115\n00:32\n\n\n3\n0.030265\n0.028458\n00:32\n\n\n4\n0.025367\n0.025913\n00:32\n\n\n5\n0.022732\n0.025986\n00:32\n\n\n\n\n\n\nSee the results.\n\nlearn.show_results(max_n=4)",
    "crumbs": [
      "Examples",
      "Unet workflow"
    ]
  },
  {
    "objectID": "data.cv.html",
    "href": "data.cv.html",
    "title": "Image data",
    "section": "",
    "text": "from geo2ml.plotting import *\nfrom matplotlib import pyplot as plt",
    "crumbs": [
      "Image data",
      "Image data"
    ]
  },
  {
    "objectID": "data.cv.html#coco-conversions",
    "href": "data.cv.html#coco-conversions",
    "title": "Image data",
    "section": "COCO conversions",
    "text": "COCO conversions\nAnnotations in COCO style datasets are stored in a single json-file, which looks like this:\n{\n    \"info\": { \n        \"year\": \"&lt;year_of_creation&gt;\",\n        \"version\": \"&lt;dataset_version&gt;\",\n        \"description\": \"&lt;description&gt;\",\n        \"contributor\": \"&lt;author&gt;\",\n        \"url\":\" &lt;url&gt;\",\n        \"date_created\": \"&lt;creation_date&gt;\"\n    },\n    \"licenses\": [\n        {\n            \"url\": \"&lt;url_to_license&gt;\",\n            \"id\": &lt;license_id_number&gt;,\n            \"name\" &lt;\"license_name&gt;\"\n        },\n        ...\n    ],\n    \"images\": [\n        {\n            \"id\": 0,\n            \"license\": \"&lt;license_id&gt;\",\n            \"file_name\": \"&lt;filename.ext&gt;\",\n            \"height\": \"&lt;height_in_px&gt;\",\n            \"width\": \"&lt;width_in_px&gt;\",\n            \"date_captured\": null\n        },\n        ...\n    ],\n    \"annotations\": [\n        {\n            \"id\": &lt;annotation_id&gt;,\n            \"image_id\": &lt;corresponding_image_id&gt;,\n            \"category_id\": &lt;corresponding_cat_id&gt;,\n            \"bbox\": [xmin, ymin, xdelta, ydelta],\n            \"segmentation\": [x0, y0, x1, y1, ...],\n            \"area\": &lt;area_in_pixels&gt;,\n            \"is_crowd\": &lt;1 or 0&gt;\n        },\n        ...\n    ],\n    \"categories\": [\n        {\n            \"id\": &lt;cat_id&gt;,\n            \"name\": \"&lt;cat_name&gt;\",\n            \"supercategory\": \"&lt;supercat_name&gt;\"\n        }\n    ]\n}\nOf these, images, annotations and categories are mandatory.\n\nsource\n\nshp_to_coco\n\n shp_to_coco (raster_path:pathlib.Path, shp_path:pathlib.Path,\n              outpath:pathlib.Path, label_col:str, coco_categories:dict,\n              coco_info:dict=None, coco_licenses:dict=None,\n              min_bbox_area:int=0, rotated_bbox:bool=False,\n              dataset_name:str=None)\n\nCreate a COCO style dataset from images in raster_path and corresponding polygons in shp_path, save annotations to outpath. shp_path can be either geopackage containing layers so that each layer corresponds to an image, or a directory containing multiple shp or geojson files, each corresponding to an image\nCreate an example dataset.\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'Fallen'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'Standing'},\n    ]\n\ncoco_info = {'description': 'dummydataset for example purposes',\n             'version': 0.1,\n             'year': 2023,\n             'contributor': 'Janne Mäyrä',\n             'date_created': datetime.date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nConvert the dataset to COCO format.\n\noutpath = Path('example_data/tiles/')\n\nshp_to_coco(raster_path=outpath/'images', shp_path=outpath/'vectors.gpkg', outpath=outpath, label_col='label',\n            dataset_name='coco_norm', coco_info=coco_info, coco_categories=deadwood_categories)\n\n\n\n\n\ndata_path = Path('example_data/tiles/')\n\nwith open(data_path/'coco_norm.json') as f:\n    coco_data = json.load(f)\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nfor i, ax in enumerate(axs.flatten()):\n    plot_coco_instance(coco_data, i, data_path/'images', ax, show_labels=False, show_title=False)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt is also possible to save the dataset with rotated bounding boxes:\n\nshp_to_coco(raster_path=outpath/'images', shp_path=outpath/'vectors', outpath=outpath, label_col='label',\n            dataset_name='coco_rot', coco_info=coco_info, coco_categories=deadwood_categories, rotated_bbox=True)\n\n\n\n\n\nwith open(data_path/'coco_rot.json') as f:\n    coco_rot_data = json.load(f)\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nfor i, ax in enumerate(axs.flatten()):\n    plot_coco_instance(coco_rot_data, i, data_path/'images', ax, show_labels=False, show_title=False)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\ncoco_to_shp\n\n coco_to_shp (coco_data:pathlib.Path|str, outpath:pathlib.Path,\n              raster_path:pathlib.Path, downsample_factor:int=1)\n\nGenerates georeferenced data from a dictionary with coco annotations. TODO handle multipolygons better\ncoco_to_shp converts COCO annotations back into georeferenced data.\n\ncoco_to_shp(outpath/'coco_norm.json', outpath/'predicted_vectors_coco', outpath/'images')\n\n\n\n\n\nfig, axs = plt.subplots(1,2, dpi=100)\norig_annos = gpd.read_file(outpath/'vectors/R0C3.geojson')\norig_annos.plot(ax=axs[0], column='label', cmap='seismic').set_title('Original annotations')\nconverted_annos = gpd.read_file(outpath/'predicted_vectors_coco/R0C3.geojson')\nconverted_annos.plot(ax=axs[1], column='label', cmap='seismic').set_title('Converted annotations')\n\nText(0.5, 1.0, 'Converted annotations')\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nshp_to_coco_results\n\n shp_to_coco_results (prediction_path:pathlib.Path,\n                      raster_path:pathlib.Path, coco_dict:pathlib.Path,\n                      outfile:pathlib.Path, label_col:str='label_id',\n                      rotated_bbox:bool=False)\n\nConvert vector predictions into coco result format to be fed into COCO evaluator\nprediction_path can be either geopackage containing layers so that each layer corresponds to an image, or a directory containing multiple shp or geojson files, each corresponding to an image\nshp_to_coco_results can be used to convert shapefiles to COCO result format, which can then be read with pycocotools.coco.COCO.loadRes and further used with pycocotools.cocoeval.COCOeval. The difference between COCO annotations and COCO results is that COCO results must be in RLE format.\n\nshp_to_coco_results(outpath/'predicted_vectors_coco', outpath/'images', outpath/'coco_norm.json', outpath/'coco_res.json',\n                    label_col='label')\nwith open(outpath/'coco_res.json') as f:\n    res = json.load(f)\nres[:3]\n\n\n\n\n[{'image_id': 0,\n  'category_id': 'Standing',\n  'segmentation': {'size': [180, 240],\n   'counts': 'Qj8f0j45N3M2N2N3L3N3M2N2N3M2N3M2N2M3N1O21O0O100O101O0O10000O2OO001O1O001O10O01OO1O3N2M4M0O2O0O2N1010O2O2M2O1N2O0O0O1O100O10O0100O1O000O101O0O10001N10000O2O02N1O2O0O1N3lNYL7i3]OdL`0Z4M3M3M3L5I6Kjfa0'},\n  'score': 0.0},\n {'image_id': 0,\n  'category_id': 'Fallen',\n  'segmentation': {'size': [180, 240],\n   'counts': 'h``01c55K4L5K4L5K5K4LO103M4L5J5L4L5K4Khdf0'},\n  'score': 0.0},\n {'image_id': 0,\n  'category_id': 'Fallen',\n  'segmentation': {'size': [180, 240], 'counts': 'lVd08\\\\59G1OO10000O:Gkad0'},\n  'score': 0.0}]",
    "crumbs": [
      "Image data",
      "Image data"
    ]
  },
  {
    "objectID": "data.cv.html#yolo",
    "href": "data.cv.html#yolo",
    "title": "Image data",
    "section": "YOLO",
    "text": "YOLO\nSpecifications of yolo format:\n\nThe folder structure must be so that images are in a folder called images and annotations in a folder labels\nEach image must have a corresponding annotation file with a same name aside from file type (txt)\nEach txt contains all annotations in separate rows\n\nBounding box annotation format is classid x_center y_center width height\nPolygon annotation format classid x y x y…\n\nCoordinates are normalized between 0 and 1, so that origin is at upper left and (1,1) in bottom right\nTrain/val/test -sets are collated in separate files, with the paths to image files\nInformation is collated on a yaml file, where\n\npath: &lt;path&gt; is the dataset root dir\ntrain:, val: and test: are either:\n\ndirectories\ntxt-files containing images\nlist containing paths\n\nclass names are saved like\n\nnames: \n    0: person\n    1: bicycle\n\n\nsource\n\nshp_to_yolo\n\n shp_to_yolo (raster_path:pathlib.Path, shp_path:pathlib.Path,\n              outpath:pathlib.Path, label_col:str, names:list,\n              ann_format:str='box', min_bbox_area:int=0,\n              dataset_name:str=None)\n\nConvert shapefiles in shp_path to YOLO style dataset. Creates a folder labels and dataset_name.yaml to outpath shp_path can be either geopackage containing layers so that each layer corresponds to an image, or a directory containing multiple shp or geojson files, each corresponding to a single image.\n\ncats = ['Standing', 'Fallen']\nshp_to_yolo(outpath/'images', outpath/'vectors', outpath, label_col='label', \n            names=cats, dataset_name='yolo_box')\n\n\n\n\n\nwith open('example_data/tiles/labels/R0C3.txt') as f:\n    print(f.read())\n\n0 0.9110088813754431 0.8428313957277873 0.2105125 0.34474444444444446\n0 0.8408608699710605 0.5332844921858052 0.1755499999999999 0.28715555555555555\n0 0.9556222313151289 0.4008342356126551 0.12364166666666669 0.5387388888888888\n0 0.2896889458001481 0.8741472392259093 0.3121958333333333 0.28441666666666665\n1 0.17423794402298073 0.49100616152477305 0.19327083333333334 0.47804444444444444\n1 0.3449062720847152 0.45305794267666205 0.21007916666666665 0.5340611111111111\n0 0.31508289921660215 0.07376972745127364 0.3144166666666666 0.20641666666666666\n0 0.5501041666666667 0.006870370370370371 0.06359166666666667 0.02061111111111111\n\n\n\n\nfig, ax = plt.subplots(1,1)\nplot_yolo_instance(Path('example_data/tiles/labels/R0C3.txt'), \n                   Path('example_data/tiles/images/R0C3.tif'), \n                   ax, ann_type='box',\n                   classes=cats)\n\n\n\n\n\n\n\n\n\nshp_to_yolo(outpath/'images', outpath/'vectors.gpkg', outpath, label_col='label', \n            ann_format='polygon', names=cats, dataset_name='yolo_poly')\n\n\n\n\n\nfig, ax = plt.subplots(1,1)\nplot_yolo_instance(Path('example_data/tiles/labels/R0C3.txt'), \n                   Path('example_data/tiles/images/R0C3.tif'), \n                   ax, ann_type='polygon',\n                   classes=cats)\n\n\n\n\n\n\n\n\n\nsource\n\n\nyolo_to_shp\n\n yolo_to_shp (prediction_path:pathlib.Path, raster_path:pathlib.Path,\n              yolo_path:pathlib.Path|str, outpath:pathlib.Path,\n              downsample_factor:int=1, ann_format:str='polygon')\n\nConvert predicted files in predictions to georeferenced data based on files in images. ann_format is one of polygon, xyxy, xywh, xyxyn, xywhn.\n\nyolo_to_shp(outpath/'labels', outpath/'images', outpath/'yolo_poly.yaml', outpath/'predicted_vectors_yolo')\n\n\n\n\n\nfig, axs = plt.subplots(1,2, dpi=100)\norig_annos = gpd.read_file(outpath/'vectors/R0C3.geojson')\norig_annos.plot(ax=axs[0], column='label', cmap='seismic').set_title('Original annotations')\nconverted_annos = gpd.read_file(outpath/'predicted_vectors_yolo/R0C3.geojson')\nconverted_annos.plot(ax=axs[1], column='label', cmap='seismic').set_title('Converted annotations')\n\nText(0.5, 1.0, 'Converted annotations')",
    "crumbs": [
      "Image data",
      "Image data"
    ]
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "# hide\nfrom nbdev.showdoc import *",
    "crumbs": [
      "Plotting"
    ]
  },
  {
    "objectID": "plotting.html#plotting-object-detection-datasets",
    "href": "plotting.html#plotting-object-detection-datasets",
    "title": "Plotting",
    "section": "Plotting object detection datasets",
    "text": "Plotting object detection datasets\n\nCOCO\n\nsource\n\n\nplot_coco_instance\n\n plot_coco_instance (coco_data:dict, image_id:int, image_dir:pathlib.Path,\n                     ax:matplotlib.axes._axes.Axes, plot_bbox:bool=True,\n                     plot_segmentation:bool=True, show_labels:bool=True,\n                     show_title:bool=True)\n\n\nimport json\n\n\nwith open('example_data/tiles/coco_norm.json') as f:\n    coco_data = json.load(f)\n\n\nfig, ax = plt.subplots(1,1)\nplot_coco_instance(coco_data, 1, Path('example_data/tiles/images/'), ax, plot_bbox=True)\nplt.show()\n\n\n\n\n\n\n\n\nAlso works with oriented bounding boxes\n\nwith open('example_data/tiles/coco_rot.json') as f:\n    coco_rot_data = json.load(f)\nfig, ax = plt.subplots(1,1)\nplot_coco_instance(coco_rot_data, 3, Path('example_data/tiles/images/'), ax, plot_bbox=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nYOLO\n\nsource\n\n\nplot_yolo_instance\n\n plot_yolo_instance (annotation_fname:pathlib.Path,\n                     image_fname:pathlib.Path,\n                     ax:matplotlib.axes._axes.Axes, ann_type:str='box',\n                     show_title:bool=True, classes:list[str]=None)\n\nPlot yolo format instance to ax and return it. If classes are provided show them in legend\n\nfig, ax = plt.subplots(1,1)\nclasses = ['standing', 'fallen']\nplot_yolo_instance(Path('example_data/tiles/labels/R0C3.txt'), Path('example_data/tiles/images/R0C3.tif'), ax, ann_type='polygon',\n                   classes=classes)",
    "crumbs": [
      "Plotting"
    ]
  },
  {
    "objectID": "data.coordinates.html",
    "href": "data.coordinates.html",
    "title": "Coordinate transformations",
    "section": "",
    "text": "source\n\nlist_to_affine\n\n list_to_affine (xform_mat:list)\n\nAdapted from Solaris.geo. Creates an affine object from array-formatted list\n\nsource\n\n\nget_geo_transform\n\n get_geo_transform (src)\n\nExtract geotransform for a raster image source\n\nsource\n\n\nconvert_poly_coords\n\n convert_poly_coords (geom:&lt;function shape&gt;, raster_src:str=None,\n                      affine_obj:affine.Affine=None, inverse:bool=False,\n                      precision=None)\n\nAdapted from solaris. Converts georeferenced coordinates to pixel coordinates and vice versa\n\nsource\n\n\naffine_transform_gdf\n\n affine_transform_gdf (gdf:geopandas.geodataframe.GeoDataFrame,\n                       affine_obj:affine.Affine, inverse:bool=False,\n                       geom_col:str='geometry', precision:int=None)\n\nAdapted from solaris, transforms all geometries in GeoDataFrame to pixel coordinates from Georeferced coordinates and vice versa\n\nsource\n\n\ngdf_to_px\n\n gdf_to_px (gdf:geopandas.geodataframe.GeoDataFrame, im_path,\n            geom_col:str='geometry', precision:int=None, outpath=None,\n            override_crs=False)\n\nAdapted from https://solaris.readthedocs.io/en/latest/_modules/solaris/vector/polygon.html#geojson_to_px_gdf Converts gdf to pixel coordinates based on image in im_path\n\nsource\n\n\ngeoregister_px_df\n\n georegister_px_df (df:pandas.core.frame.DataFrame, im_path=None,\n                    affine_obj:affine.Affine=None, crs=None,\n                    geom_col:str='geometry', precision:int=None,\n                    output_path=None)\n\nConvert geodataframe from pixel coordinates to crs, using affine_obj as the reference\n\ngdf = gpd.read_file('example_data/R70C21.shp')\nim_path = 'example_data/R70C21.tif'\n\nwith rio.open(im_path) as im: \n    im_data = im.read()\nplt.imshow(im_data.swapaxes(0,2).swapaxes(0,1))\nplt.show()\n\n\n\n\n\n\n\n\n\ngdf.plot(column='label', #facecolor='none', \n         edgecolor='black', lw=0.7, cmap='viridis')\n\n\n\n\n\n\n\n\n\ntfmd_gdf = gdf_to_px(gdf, im_path)\ntfmd_gdf.plot(column='label', #facecolor='none', \n              edgecolor='black', lw=0.7, cmap='viridis')\n\n\n\n\n\n\n\n\nOrigin in lower left for this data.\n\nfig, ax = plt.subplots()\nax.imshow(im_data.swapaxes(0,2).swapaxes(0,1))\ntfmd_gdf.plot(ax=ax, column='label', facecolor='none', edgecolor='black', lw=0.7, cmap='viridis')\nplt.show()\n\n\n\n\n\n\n\n\nOverlaid on image coordinates are correct\n\ntfmd_gdf = georegister_px_df(tfmd_gdf, im_path)\ntfmd_gdf.plot(column='label', facecolor='none', edgecolor='black', lw=0.7, cmap='viridis')\n\n\n\n\n\n\n\n\nBackwards transformation works",
    "crumbs": [
      "Image data",
      "Coordinate transformations"
    ]
  },
  {
    "objectID": "examples_tabular.html",
    "href": "examples_tabular.html",
    "title": "Tabular data workflow",
    "section": "",
    "text": "import rasterio as rio\nimport geopandas as gpd\nfrom pathlib import Path\nimport rasterio.plot as rioplot\nimport matplotlib.pyplot as plt\n\n\npath_to_data = Path('workflow_examples/')\nraster_data = path_to_data/'s2_2018_lataseno.tif'\npoint_data = path_to_data/'points_clc.geojson'\n\nExample data here are Sentinel 2 mosaic from 2018, with 9 bands, and scattered point observations from that area. Target class for the points is Corine Land Cover class for the corresponding location\n\nfig, axs = plt.subplots(1,2, dpi=100, figsize=(10,4))\nwith rio.open(raster_data) as src:\n    rioplot.show((src, (3,2,1)), adjust=True, ax=axs[0])\ntrain_gdf = gpd.read_file(point_data)\ntrain_gdf['corine'] = train_gdf.corine.astype('category')\ntrain_gdf.plot(column='corine', ax=axs[1], cmap='tab20', markersize=1, legend=True,\n               legend_kwds={'loc':'right', 'bbox_to_anchor':(1.2,0.5)})\nplt.suptitle('Example area')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ngeo2ml_sample_points \\\nexample_data/workflow_examples/points_clc.geojson \\\nexample_data/workflow_examples/s2_2018_lataseno.tif \\\ncorine \\\nexample_data/workflow_examples/points/ \\\n--out_prefix example\n\n\n\n\nfrom geo2ml.scripts.data import sample_points\n\n\noutpath = path_to_data/'points'\n\nsample_points(point_data, raster_data, 'corine', outpath, out_prefix='example')\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n\ndf = pd.read_csv(outpath/'example__s2_2018_lataseno__points_clc__corine.csv')\n\ny = df['corine']\nX = df.drop(columns='corine')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n              precision    recall  f1-score   support\n\n          23       0.55      0.77      0.64        81\n          24       0.00      0.00      0.00         7\n          25       0.00      0.00      0.00         7\n          28       0.00      0.00      0.00         2\n          32       0.42      0.36      0.39        36\n          33       0.00      0.00      0.00         2\n          34       0.00      0.00      0.00         2\n          35       0.00      0.00      0.00         3\n          39       0.00      0.00      0.00         1\n          40       0.00      0.00      0.00         2\n          42       0.00      0.00      0.00         1\n          43       0.70      0.72      0.71        75\n          47       1.00      0.50      0.67         2\n          48       0.83      0.83      0.83         6\n\n    accuracy                           0.59       227\n   macro avg       0.25      0.23      0.23       227\nweighted avg       0.53      0.59      0.55       227",
    "crumbs": [
      "Examples",
      "Tabular data workflow"
    ]
  },
  {
    "objectID": "examples_tabular.html#point-based",
    "href": "examples_tabular.html#point-based",
    "title": "Tabular data workflow",
    "section": "",
    "text": "import rasterio as rio\nimport geopandas as gpd\nfrom pathlib import Path\nimport rasterio.plot as rioplot\nimport matplotlib.pyplot as plt\n\n\npath_to_data = Path('workflow_examples/')\nraster_data = path_to_data/'s2_2018_lataseno.tif'\npoint_data = path_to_data/'points_clc.geojson'\n\nExample data here are Sentinel 2 mosaic from 2018, with 9 bands, and scattered point observations from that area. Target class for the points is Corine Land Cover class for the corresponding location\n\nfig, axs = plt.subplots(1,2, dpi=100, figsize=(10,4))\nwith rio.open(raster_data) as src:\n    rioplot.show((src, (3,2,1)), adjust=True, ax=axs[0])\ntrain_gdf = gpd.read_file(point_data)\ntrain_gdf['corine'] = train_gdf.corine.astype('category')\ntrain_gdf.plot(column='corine', ax=axs[1], cmap='tab20', markersize=1, legend=True,\n               legend_kwds={'loc':'right', 'bbox_to_anchor':(1.2,0.5)})\nplt.suptitle('Example area')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ngeo2ml_sample_points \\\nexample_data/workflow_examples/points_clc.geojson \\\nexample_data/workflow_examples/s2_2018_lataseno.tif \\\ncorine \\\nexample_data/workflow_examples/points/ \\\n--out_prefix example\n\n\n\n\nfrom geo2ml.scripts.data import sample_points\n\n\noutpath = path_to_data/'points'\n\nsample_points(point_data, raster_data, 'corine', outpath, out_prefix='example')\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n\ndf = pd.read_csv(outpath/'example__s2_2018_lataseno__points_clc__corine.csv')\n\ny = df['corine']\nX = df.drop(columns='corine')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n              precision    recall  f1-score   support\n\n          23       0.55      0.77      0.64        81\n          24       0.00      0.00      0.00         7\n          25       0.00      0.00      0.00         7\n          28       0.00      0.00      0.00         2\n          32       0.42      0.36      0.39        36\n          33       0.00      0.00      0.00         2\n          34       0.00      0.00      0.00         2\n          35       0.00      0.00      0.00         3\n          39       0.00      0.00      0.00         1\n          40       0.00      0.00      0.00         2\n          42       0.00      0.00      0.00         1\n          43       0.70      0.72      0.71        75\n          47       1.00      0.50      0.67         2\n          48       0.83      0.83      0.83         6\n\n    accuracy                           0.59       227\n   macro avg       0.25      0.23      0.23       227\nweighted avg       0.53      0.59      0.55       227",
    "crumbs": [
      "Examples",
      "Tabular data workflow"
    ]
  },
  {
    "objectID": "examples_tabular.html#polygon-based",
    "href": "examples_tabular.html#polygon-based",
    "title": "Tabular data workflow",
    "section": "Polygon-based",
    "text": "Polygon-based\nExample data here is RGB UAV data from Evo, Hämeenlinna, and the target polygons are tree canopies. Target class is the species (Spruce, pine, birch, aspen) or standing deadwood, found on column label. As the extracted features, we use min, max, mean, std and median of the red, green and blue channels within the canopies.\n\nuav_data = path_to_data/'example_area.tif'\ncanopy_data = path_to_data/'canopies.geojson'\n\nfig, axs = plt.subplots(1,2, dpi=100, figsize=(5,7))\nwith rio.open(uav_data) as src:\n    rioplot.show((src, (1,2,3)), adjust=True, ax=axs[0])\ntrain_gdf = gpd.read_file(canopy_data)\ntrain_gdf.plot(column='label', ax=axs[1], cmap='tab20')\nplt.suptitle('Example area')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCLI\ngeo2ml_sample_polygons \\\nexample_data/workflow_examples/canopies.geojson \\\nexample_data/workflow_examples/example_area.tif \\\nlabel \\\nexample_data/workflow_examples/polygons/ \\\n--out_prefix example \\\n--min --max --mean --std --median\n\nPython\n\nfrom geo2ml.scripts.data import sample_polygons\n\n\noutpath = path_to_data/'polygons'\n\nsample_polygons(canopy_data, uav_data, 'label', outpath, out_prefix='example',\n                min=True, max=True, mean=True, std=True, median=True,\n                count=False, sum=False, categorical=False)\n\n/home/mayrajeo/miniconda3/envs/point-eo-dev/lib/python3.11/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\nDataset structure\n\n\nTrain model\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\ndf = pd.read_csv(outpath/'example__example_area__canopies__label.csv')\n\nle = LabelEncoder()\ny = le.fit_transform(df['label'])\nX = df.drop(columns='label')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred, zero_division=0, target_names=le.classes_))\n\n                   precision    recall  f1-score   support\n\n            Birch       0.67      0.59      0.63       326\n   European aspen       0.85      0.50      0.63       102\n    Norway spruce       0.83      0.93      0.87       777\n       Scots pine       0.84      0.78      0.81       273\nStanding deadwood       0.96      0.96      0.96        74\n\n         accuracy                           0.81      1552\n        macro avg       0.83      0.75      0.78      1552\n     weighted avg       0.80      0.81      0.80      1552\n\n\n\n\ncm = confusion_matrix(y_test, y_pred, labels=rf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\ndisp.plot()\nplt.show()",
    "crumbs": [
      "Examples",
      "Tabular data workflow"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "geo2ml",
    "section": "",
    "text": "Sampling features from a raster using point geometries or polygons\nTiling larger rasters and shapefiles into smaller patches\nRasterizing polygon geometries for semantic segmentation tasks\nConverting vector data to COCO and YOLO formats and creating required dataset files\nVisualization of generated datasets",
    "crumbs": [
      "geo2ml"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "geo2ml",
    "section": "Install",
    "text": "Install\nFirst install GDAL to your system. If you use conda then installing rasterio is enough, but with pip use instructions from https://pypi.org/project/GDAL/.\nThen you can install the package by\npip install git+git://github.com/mayrajeo/geo2ml.git\nIf you want to have an editable install then first clone the repository\ngit clone https://github.com/mayrajeo/geo2ml.git\ncd geo2ml\npip install -e .",
    "crumbs": [
      "geo2ml"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "geo2ml",
    "section": "How to use",
    "text": "How to use\nRunning geo2ml_help shows all available commands. Documentation for each command is found by running for example geo2ml_sample_points --help",
    "crumbs": [
      "geo2ml"
    ]
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "cli",
    "section": "",
    "text": "source\n\nchelp\n\n chelp ()\n\nShow help for all console scripts"
  },
  {
    "objectID": "data.tabular.html",
    "href": "data.tabular.html",
    "title": "Tabular data",
    "section": "",
    "text": "example_data_path= Path('example_data')\nexample_points = example_data_path/'points/points.geojson'\nexample_polys = example_data_path/'polygons/polygons.geojson'\nexample_raster = example_data_path/'s2_lataseno_ex.tif'",
    "crumbs": [
      "Tabular data",
      "Tabular data"
    ]
  },
  {
    "objectID": "data.tabular.html#data-conversion-and-processing",
    "href": "data.tabular.html#data-conversion-and-processing",
    "title": "Tabular data",
    "section": "Data conversion and processing",
    "text": "Data conversion and processing\nUtility functions to process dataframes.\n\nsource\n\narray_to_longform\n\n array_to_longform (a:pandas.core.frame.DataFrame, columns:list)\n\nConvert pd.DataFrame a to longform array\n\nsource\n\n\ndrop_small_classes\n\n drop_small_classes (df:pandas.core.frame.DataFrame, min_class_size:int,\n                     target_column:str|int=0)\n\nDrop rows from the dataframe if their target_column value has less instances than `min_class_size\nGenerate random data to test.\n\nex_df = pd.DataFrame({'label': np.random.randint(1, 10, 200)})\nex_df.label.value_counts()\n\nlabel\n4    32\n3    23\n2    22\n9    21\n1    21\n6    21\n5    21\n8    21\n7    18\nName: count, dtype: int64\n\n\nColumn name can be either specified with string or int. If not provided it defaults to first column.\n\nfiltered = drop_small_classes(ex_df, 20, 'label')\nassert filtered.label.value_counts().min() &gt;= 20\nfiltered.label.value_counts()\n\nlabel\n4    32\n3    23\n2    22\n9    21\n1    21\n6    21\n5    21\n8    21\nName: count, dtype: int64\n\n\nIf not specified, defaults to first column.\n\nfiltered = drop_small_classes(ex_df, 20)\nfiltered.label.value_counts()\n\nlabel\n4    32\n3    23\n2    22\n9    21\n1    21\n6    21\n5    21\n8    21\nName: count, dtype: int64",
    "crumbs": [
      "Tabular data",
      "Tabular data"
    ]
  },
  {
    "objectID": "data.tabular.html#sampling-utilities",
    "href": "data.tabular.html#sampling-utilities",
    "title": "Tabular data",
    "section": "Sampling utilities",
    "text": "Sampling utilities\nThese functions enable sampling of raster values using either point or polygon features.\n\nsource\n\nsample_raster_with_points\n\n sample_raster_with_points (sampling_locations:pathlib.Path,\n                            input_raster:pathlib.Path, target_column:str,\n                            gpkg_layer:str=None,\n                            band_names:list[str]=None,\n                            rename_target:str=None)\n\nExtract values from input_raster using points from sampling_locations. Returns a gpd.GeoDataFrame with columns target_column, geometry and bands\nsample_raster_with_points is an utility to sample point values from a raster and get the results into a gpd.GeoDataFrame.\n\nout_gdf = sample_raster_with_points(example_points, example_raster, 'id')\nout_gdf.head()\n\n\n\n\n\n\n\n\n\nid\ngeometry\nband_0\nband_1\nband_2\nband_3\nband_4\nband_5\nband_6\nband_7\nband_8\n\n\n\n\n0\n10.0\nPOINT (311760.599 7604880.391)\n334\n591\n439\n1204\n2651\n3072\n3177\n2070\n1046\n\n\n1\n14.0\nPOINT (312667.464 7605426.442)\n183\n359\n282\n759\n1742\n2002\n2037\n1392\n669\n\n\n2\n143.0\nPOINT (313619.160 7604550.762)\n281\n478\n427\n900\n1976\n2315\n2423\n2139\n1069\n\n\n3\n172.0\nPOINT (311989.967 7605411.190)\n287\n530\n393\n1078\n2446\n2761\n2978\n1949\n950\n\n\n4\n224.0\nPOINT (313386.009 7604304.917)\n204\n379\n327\n753\n1524\n1747\n1771\n1322\n663\n\n\n\n\n\n\n\n\nIt is also possible to provide band_names to rename the columns.\n\nband_names = ['blue', 'green', 'red', 'red_edge1', 'red_edge2', 'nir', 'narrow_nir', 'swir1', 'swir2']\nout_gdf = sample_raster_with_points(example_points, example_raster, 'id', band_names=band_names)\nout_gdf.head()\n\n\n\n\n\n\n\n\n\nid\ngeometry\nblue\ngreen\nred\nred_edge1\nred_edge2\nnir\nnarrow_nir\nswir1\nswir2\n\n\n\n\n0\n10.0\nPOINT (311760.599 7604880.391)\n334\n591\n439\n1204\n2651\n3072\n3177\n2070\n1046\n\n\n1\n14.0\nPOINT (312667.464 7605426.442)\n183\n359\n282\n759\n1742\n2002\n2037\n1392\n669\n\n\n2\n143.0\nPOINT (313619.160 7604550.762)\n281\n478\n427\n900\n1976\n2315\n2423\n2139\n1069\n\n\n3\n172.0\nPOINT (311989.967 7605411.190)\n287\n530\n393\n1078\n2446\n2761\n2978\n1949\n950\n\n\n4\n224.0\nPOINT (313386.009 7604304.917)\n204\n379\n327\n753\n1524\n1747\n1771\n1322\n663\n\n\n\n\n\n\n\n\nOr rename target column\n\nout_gdf = sample_raster_with_points(example_points, example_raster, 'id', rename_target='target')\nout_gdf.head()\n\n\n\n\n\n\n\n\n\ntarget\ngeometry\nband_0\nband_1\nband_2\nband_3\nband_4\nband_5\nband_6\nband_7\nband_8\n\n\n\n\n0\n10.0\nPOINT (311760.599 7604880.391)\n334\n591\n439\n1204\n2651\n3072\n3177\n2070\n1046\n\n\n1\n14.0\nPOINT (312667.464 7605426.442)\n183\n359\n282\n759\n1742\n2002\n2037\n1392\n669\n\n\n2\n143.0\nPOINT (313619.160 7604550.762)\n281\n478\n427\n900\n1976\n2315\n2423\n2139\n1069\n\n\n3\n172.0\nPOINT (311989.967 7605411.190)\n287\n530\n393\n1078\n2446\n2761\n2978\n1949\n950\n\n\n4\n224.0\nPOINT (313386.009 7604304.917)\n204\n379\n327\n753\n1524\n1747\n1771\n1322\n663\n\n\n\n\n\n\n\n\n\nsource\n\n\nsample_raster_with_polygons\n\n sample_raster_with_polygons (sampling_locations:pathlib.Path,\n                              input_raster:pathlib.Path,\n                              target_column:str=None, gpkg_layer:str=None,\n                              band_names:list[str]=None,\n                              rename_target:str=None,\n                              stats:list[str]=['min', 'max', 'mean',\n                              'count'], categorical:bool=False)\n\nExtract values from input_raster using polygons from sampling_locations with rasterstats.zonal_stats for all bands\nExample polygons here are previous points buffered by 40 meters.\n\nout_gdf = sample_raster_with_polygons(example_polys, example_raster, 'id')\nout_gdf.iloc[0]\n\nid                                                           10.0\ngeometry        MULTIPOLYGON (((311800.59915342694 7604880.390...\nband_0_min                                                  266.0\nband_0_max                                                  415.0\nband_0_mean                                            335.708333\nband_0_count                                                   48\nband_1_min                                                  351.0\nband_1_max                                                  696.0\nband_1_mean                                              582.1875\nband_1_count                                                   48\nband_2_min                                                  412.0\nband_2_max                                                  699.0\nband_2_mean                                            524.520833\nband_2_count                                                   48\nband_3_min                                                  885.0\nband_3_max                                                 1462.0\nband_3_mean                                              1237.125\nband_3_count                                                   48\nband_4_min                                                 1310.0\nband_4_max                                                 2888.0\nband_4_mean                                           2479.291667\nband_4_count                                                   48\nband_5_min                                                 1565.0\nband_5_max                                                 3317.0\nband_5_mean                                               2880.25\nband_5_count                                                   48\nband_6_min                                                 1579.0\nband_6_max                                                 3665.0\nband_6_mean                                           3127.166667\nband_6_count                                                   48\nband_7_min                                                 1860.0\nband_7_max                                                 2214.0\nband_7_mean                                           2076.895833\nband_7_count                                                   48\nband_8_min                                                 1024.0\nband_8_max                                                 1144.0\nband_8_mean                                              1075.375\nband_8_count                                                   48\nName: 0, dtype: object\n\n\nAs sample_raster_with_polygons utilizes rasterstats.zonal_statistics, all stats supported by it can be provided with parameter stats. More information here.\n\nout_gdf = sample_raster_with_polygons(example_polys, example_raster, 'id', stats=['min', 'max', 'sum', 'median', 'range'])\nout_gdf.iloc[0]\n\nid                                                            10.0\ngeometry         MULTIPOLYGON (((311800.59915342694 7604880.390...\nband_0_min                                                   266.0\nband_0_max                                                   415.0\nband_0_sum                                                 16114.0\nband_0_median                                                338.0\nband_0_range                                                 149.0\nband_1_min                                                   351.0\nband_1_max                                                   696.0\nband_1_sum                                                 27945.0\nband_1_median                                                590.0\nband_1_range                                                 345.0\nband_2_min                                                   412.0\nband_2_max                                                   699.0\nband_2_sum                                                 25177.0\nband_2_median                                                524.0\nband_2_range                                                 287.0\nband_3_min                                                   885.0\nband_3_max                                                  1462.0\nband_3_sum                                                 59382.0\nband_3_median                                               1255.5\nband_3_range                                                 577.0\nband_4_min                                                  1310.0\nband_4_max                                                  2888.0\nband_4_sum                                                119006.0\nband_4_median                                               2625.5\nband_4_range                                                1578.0\nband_5_min                                                  1565.0\nband_5_max                                                  3317.0\nband_5_sum                                                138252.0\nband_5_median                                               3020.5\nband_5_range                                                1752.0\nband_6_min                                                  1579.0\nband_6_max                                                  3665.0\nband_6_sum                                                150104.0\nband_6_median                                               3270.5\nband_6_range                                                2086.0\nband_7_min                                                  1860.0\nband_7_max                                                  2214.0\nband_7_sum                                                 99691.0\nband_7_median                                               2087.0\nband_7_range                                                 354.0\nband_8_min                                                  1024.0\nband_8_max                                                  1144.0\nband_8_sum                                                 51618.0\nband_8_median                                               1070.0\nband_8_range                                                 120.0\nName: 0, dtype: object",
    "crumbs": [
      "Tabular data",
      "Tabular data"
    ]
  },
  {
    "objectID": "scripts.data.html",
    "href": "scripts.data.html",
    "title": "Dataset creation",
    "section": "",
    "text": "geo2ml provides the following commands for creating datasets from geospatial raster and vector data.\n\ngeo2ml_sample_points\ngeo2ml_sample_polygons\ngeo2ml_create_raster_dataset\ngeo2ml_create_yolo_dataset\ngeo2ml_create_coco_dataset\n\nThese commands can be either used from CLI using geo2ml_ -prefixed commands, or used in python scripts or notebooks like\nfrom geo2ml.scripts.data import sample_points\n\nsampling_locations = Path(&lt;path_to_locations&gt;)\ninput_raster = Path(&lt;path_to_raster&gt;)\ntarget_column = 'column'\noutpath = Path(&lt;path_to_save_files&gt;)\n\nsample_points(sampling_locations, input_raster, target_column, outpath)",
    "crumbs": [
      "CLI",
      "Dataset creation"
    ]
  },
  {
    "objectID": "scripts.data.html#overview",
    "href": "scripts.data.html#overview",
    "title": "Dataset creation",
    "section": "",
    "text": "geo2ml provides the following commands for creating datasets from geospatial raster and vector data.\n\ngeo2ml_sample_points\ngeo2ml_sample_polygons\ngeo2ml_create_raster_dataset\ngeo2ml_create_yolo_dataset\ngeo2ml_create_coco_dataset\n\nThese commands can be either used from CLI using geo2ml_ -prefixed commands, or used in python scripts or notebooks like\nfrom geo2ml.scripts.data import sample_points\n\nsampling_locations = Path(&lt;path_to_locations&gt;)\ninput_raster = Path(&lt;path_to_raster&gt;)\ntarget_column = 'column'\noutpath = Path(&lt;path_to_save_files&gt;)\n\nsample_points(sampling_locations, input_raster, target_column, outpath)",
    "crumbs": [
      "CLI",
      "Dataset creation"
    ]
  },
  {
    "objectID": "scripts.data.html#tabular-datasets",
    "href": "scripts.data.html#tabular-datasets",
    "title": "Dataset creation",
    "section": "Tabular datasets",
    "text": "Tabular datasets\nBoth of these commands create a dataset by sampling point or polygon values provided in sampling_locations from input_raster and save the resulting table as a csv and geojson or shapfile to outpath.\n\nsource\n\nsample_points\n\n sample_points (sampling_locations:pathlib.Path,\n                input_raster:pathlib.Path, target_column:str,\n                outpath:pathlib.Path, gpkg_layer:str=None,\n                save_as_shp:bool=False, rename_target:str=None,\n                band_names:pathlib.Path=None, dropna_value:int=None,\n                out_prefix:str='')\n\nSample pixel values from input_raster using sampling_locations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsampling_locations\nPath\n\nPath to the geojson/shapefile containing the sampling locations as points\n\n\ninput_raster\nPath\n\nPath to the raster used for sampling\n\n\ntarget_column\nstr\n\nColumn of sampling_locations used as the target\n\n\noutpath\nPath\n\nPath to save the output files. Is created if doesn’t exist\n\n\ngpkg_layer\nstr\nNone\nIf sampling_locations is .gpkg, specify the layer used. Ignored otherwise.\n\n\nsave_as_shp\nbool\nFalse\nSave results as shapefiles? If False, saves as geojson\n\n\nrename_target\nstr\nNone\nIf provided, target column is renamed to this\n\n\nband_names\nPath\nNone\nPath to a file providing bands to use as rows\n\n\ndropna_value\nint\nNone\nDrop all rows with all values equal to this value\n\n\nout_prefix\nstr\n\nPrefix for outputs\n\n\n\n\nsource\n\n\nsample_polygons\n\n sample_polygons (sampling_locations:pathlib.Path,\n                  input_raster:pathlib.Path, target_column:str,\n                  outpath:pathlib.Path, min:bool, max:bool, mean:bool,\n                  count:bool, sum:bool, std:bool, median:bool,\n                  categorical:bool=False, gpkg_layer:str=None,\n                  save_as_shp:bool=False, rename_target:str=None,\n                  band_names:pathlib.Path=None, dropna_value:int=None,\n                  out_prefix:str='')\n\nSample pixel values from input_raster using sampling_locations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsampling_locations\nPath\n\nPath to the geojson/shapefile containing the sampling locations as polygons\n\n\ninput_raster\nPath\n\nPath to the raster used for sampling\n\n\ntarget_column\nstr\n\nColumn of sampling_locations used for sampling\n\n\noutpath\nPath\n\nPath to save the output files. Is created if doesn’t exist\n\n\nmin\nbool\n\nCompute minimum\n\n\nmax\nbool\n\nCompute maximum\n\n\nmean\nbool\n\nCompute mean\n\n\ncount\nbool\n\nCompute count\n\n\nsum\nbool\n\nCompute sum\n\n\nstd\nbool\n\nCompute standard deviation\n\n\nmedian\nbool\n\nCompute median\n\n\ncategorical\nbool\nFalse\nAre bands categorical data?\n\n\ngpkg_layer\nstr\nNone\nIf sampling_locations is .gpkg, specify the layer used. Ignored otherwise.\n\n\nsave_as_shp\nbool\nFalse\nSave results as shapefiles? If False, saves as geojson\n\n\nrename_target\nstr\nNone\nIf provided, target column is renamed to this\n\n\nband_names\nPath\nNone\nPath to a file providing bands to use as rows\n\n\ndropna_value\nint\nNone\nDrop all rows with all values equal to this value\n\n\nout_prefix\nstr\n\nPrefix for outputs",
    "crumbs": [
      "CLI",
      "Dataset creation"
    ]
  },
  {
    "objectID": "scripts.data.html#computer-vision-dataset-creation",
    "href": "scripts.data.html#computer-vision-dataset-creation",
    "title": "Dataset creation",
    "section": "Computer vision dataset creation",
    "text": "Computer vision dataset creation\n\nsource\n\ncreate_raster_dataset\n\n create_raster_dataset (raster_path:pathlib.Path, mask_path:pathlib.Path,\n                        outpath:pathlib.Path, save_grid:bool=False,\n                        allow_partial_data:bool=False,\n                        keep_bg_only:bool=False, target_column:str=None,\n                        gpkg_layer:str=None, gridsize_x:int=256,\n                        gridsize_y:int=256, overlap_x:int=0,\n                        overlap_y:int=0)\n\nCreate a semantic segmentation dataset from a raster_path and corresponding mask mask_path. Raster image patches are saved to outpath/raster_tiles and mask patches to outpath/mask_tiles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraster_path\nPath\n\nPath to a raster that is the base for the dataset\n\n\nmask_path\nPath\n\nPath to corresponding mask raster or polygon layer. Must have the same extent and resolution as the raster in raster_path\n\n\noutpath\nPath\n\nWhere to save the results\n\n\nsave_grid\nbool\nFalse\nWhether to save the tiling grid\n\n\nallow_partial_data\nbool\nFalse\nWhether to create tiles that have only partial data\n\n\nkeep_bg_only\nbool\nFalse\nKeep the mask chips that contain only the background class\n\n\ntarget_column\nstr\nNone\nIf mask_path contains vector data, identifier of the column containing the class information\n\n\ngpkg_layer\nstr\nNone\nIf polygon_path is a geopackage, specify the layer used. Ignored otherwise.\n\n\ngridsize_x\nint\n256\nSize of tiles in x-axis in pixels\n\n\ngridsize_y\nint\n256\nSize of tiles in y-axis in pixels\n\n\noverlap_x\nint\n0\nOverlap of tiles in x-axis in pixels\n\n\noverlap_y\nint\n0\nOverlap of tiles in y-axis in pixels\n\n\n\n\nsource\n\n\ncreate_coco_dataset\n\n create_coco_dataset (raster_path:pathlib.Path, polygon_path:pathlib.Path,\n                      target_column:str, outpath:pathlib.Path,\n                      dataset_name:str, gpkg_layer:str=None,\n                      min_area_pct:float=0.0, output_format:str='geojson',\n                      save_grid:bool=False, allow_partial_data:bool=False,\n                      gridsize_x:int=320, gridsize_y:int=320,\n                      overlap_x:int=0, overlap_y:int=0,\n                      ann_format:str='box', min_bbox_area:int=0)\n\nCreate a COCO-format dataset from raster and polygon shapefile\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraster_path\nPath\n\nPath to a raster that is the base for the dataset\n\n\npolygon_path\nPath\n\nPath to annotated polygons\n\n\ntarget_column\nstr\n\nWhich column contains class information\n\n\noutpath\nPath\n\nWhere to save the resulting files\n\n\ndataset_name\nstr\n\nName of the dataset\n\n\ngpkg_layer\nstr\nNone\nIf polygon_path is a geopackage, specify the layer used. Ignored otherwise.\n\n\nmin_area_pct\nfloat\n0.0\nHow small polygons keep after tiling?\n\n\noutput_format\nstr\ngeojson\nWhich format to use for saving, either ‘geojson’ or ‘gpkg’\n\n\nsave_grid\nbool\nFalse\nShould tiling grid be saved\n\n\nallow_partial_data\nbool\nFalse\nWhether to create tiles that have only partial image data\n\n\ngridsize_x\nint\n320\nSize of tiles in x-axis in pixels\n\n\ngridsize_y\nint\n320\nSize of tiles in y-axis in pixels\n\n\noverlap_x\nint\n0\nOverlap of tiles in x-axis in pixels\n\n\noverlap_y\nint\n0\nOverlap of tiles in y-axis in pixels\n\n\nann_format\nstr\nbox\nAnnotation format, either box, polygon or rotated box\n\n\nmin_bbox_area\nint\n0\nMinimum bounding gox area in pixels. Smaller objects than this are discarded\n\n\n\n\nsource\n\n\ncreate_yolo_dataset\n\n create_yolo_dataset (raster_path:pathlib.Path, polygon_path:pathlib.Path,\n                      target_column:str, outpath:pathlib.Path,\n                      dataset_name:str=None, gpkg_layer:str=None,\n                      min_area_pct:float=0.0, output_format:str='geojson',\n                      save_grid:bool=False, allow_partial_data:bool=False,\n                      gridsize_x:int=320, gridsize_y:int=320,\n                      overlap_x:int=0, overlap_y:int=0,\n                      ann_format:str='box', min_bbox_area:int=0)\n\nCreate a YOLO-format dataset from raster and polygon shapefile\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraster_path\nPath\n\nPath to a raster that is the base for the dataset\n\n\npolygon_path\nPath\n\nPath to annotated polygons\n\n\ntarget_column\nstr\n\nWhich column contains class information\n\n\noutpath\nPath\n\nWhere to save the resulting files?\n\n\ndataset_name\nstr\nNone\nOptional name of the dataset\n\n\ngpkg_layer\nstr\nNone\nIf polygon_path is a geopackage, specify the layer used. Ignored otherwise.\n\n\nmin_area_pct\nfloat\n0.0\nHow small polygons keep after tiling?\n\n\noutput_format\nstr\ngeojson\nWhich format to use for saving, either ‘geojson’ or ‘gpkg’\n\n\nsave_grid\nbool\nFalse\nShould tiling grid be saved\n\n\nallow_partial_data\nbool\nFalse\nWhether to create tiles that have only partial image data\n\n\ngridsize_x\nint\n320\nSize of tiles in x-axis, pixels\n\n\ngridsize_y\nint\n320\nSize fo tiles in y-axis, pixels\n\n\noverlap_x\nint\n0\nOverlap of tiles in x-axis\n\n\noverlap_y\nint\n0\nOverlap of tiles in y-axis\n\n\nann_format\nstr\nbox\nAnnotation format, either box, polygon or rotated box\n\n\nmin_bbox_area\nint\n0\nMinimum bounding box area in pixels. Smaller objects than this are discarded",
    "crumbs": [
      "CLI",
      "Dataset creation"
    ]
  }
]